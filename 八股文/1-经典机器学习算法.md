## 介绍一下随机梯度下降

> 中兴算法岗一面

​     随机梯度下降（SGD）是一种通过随机采样单个样本或小批量数据来计算梯度并更新模型参数的优化算法，它避免了传统批量梯度下降中需要遍历全体数据的高计算开销，特别适合处理大规模数据集和深度学习场景。SGD的核心优势在于高效性和灵活性——单次迭代速度快、内存占用低，且梯度噪声带来的随机性有助于模型跳出局部最优解，但也存在更新方向不稳定、收敛路径震荡以及对学习率敏感等问题。实际应用中常通过引入动量（加速收敛、减少震荡）、自适应学习率（如Adam动态调整参数更新幅度）或学习率调度策略（如预热和余弦退火）来优化其性能。当前SGD及其变体（如分布式SGD、LAMB优化器）仍是训练神经网络、推荐系统和自然语言处理模型的主流方法，尤其在超大规模模型训练中，通过梯度裁剪、权重衰减等技巧进一步提升稳定性和泛化能力。



## 逻辑回归与线性回归

### 什么是逻辑回归？

逻辑回归是一种基于概率的线性分类模型，主要用于解决二分类问题（如预测用户是否点击广告或患病风险），其核心通过Sigmoid函数将线性组合 $y = \theta^T x$ 映射到[0,1]区间，输出样本属于正类的概率（例如“用户点击概率为80%”），训练时通过极大似然估计和交叉熵损失函数优化参数，支持L1/L2正则化防止过拟合，优势在于计算高效、可解释性强（权重直接反映特征影响程度）

- **Sigmoid函数** 由于线性回归的结果范围为正负无穷，因此通过**对数几率**将线性回归的**预测结果**非线性映射到固定区间(0~1)之间，数学表达式为: $\text{Sigmod}(z) = \cfrac{1}{1+e^{-z}}$

逻辑回归的极大似然估计（MLE）通过最大化样本数据出现的概率来求解参数：首先假设标签服从伯努利分布，构建似然函数$L(w)=\prod \sigma(w^T x_i)^{y_i}(1-\sigma(w^T x_i))^{1-y_i}$，取对数转化为交叉熵损失$J(w)=-\frac{1}{N}\sum [y_i\log\sigma(w^T x_i)+(1-y_i)\log(1-\sigma(w^T x_i))]$，其梯度为$\nabla_w J=\frac{1}{N}\sum (\sigma(w^T x_i)-y_i)x_i$，通过梯度下降（如SGD、Adam）或牛顿法（利用Hessian矩阵加速收敛）迭代优化参数；为提升泛化能力，常加入L1/L2正则化约束权重，前者稀疏化特征，后者抑制过拟合。实际应用中需特征标准化、参数初始化为近零值，并监控验证损失防止早停，其高效线性计算与概率可解释性使其在金融风控、医疗诊断等场景中不可替代。

最大化对数似然等价于最小化**交叉熵损失函数**



### 逻辑回归为什么不用最小二乘？

逻辑回归不使用最小二乘法（OLS）的核心原因在于模型假设与优化目标的本质冲突：OLS假设误差服从正态分布且输出为连续值，而逻辑回归建模的是二分类概率（伯努利分布），若强行使用OLS，会导致损失函数因Sigmoid非线性映射而非凸（存在多个局部极小），且梯度计算中包含$\sigma'(z)$项（当预测接近0/1时梯度消失），难以有效更新参数；此外，OLS的均方误差对极端概率预测（如真实标签1但预测概率0.9）惩罚不足（残差平方仅0.01），而交叉熵损失通过$\log$函数对错误预测施加指数级惩罚（如$-\log(0.1)=2.3$），更适配概率建模需求，同时MLE（极大似然估计）保证了参数估计的一致性，避免概率值超出[0,1]的合理范围，而OLS可能产生荒谬预测（如概率120%）。因此，逻辑回归选择交叉熵损失是理论适配性、优化稳定性与业务解释性的综合最优解。





### 逻辑回归缓解过拟合的方法





### 逻辑回归采用的是交叉熵，那你知道相对熵（KL）吗？
