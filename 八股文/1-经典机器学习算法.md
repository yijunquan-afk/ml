

## 基本概念

### 介绍一下随机梯度下降

> 中兴算法岗一面

​     随机梯度下降（SGD）是一种通过随机采样单个样本或小批量数据来计算梯度并更新模型参数的优化算法，它避免了传统批量梯度下降中需要遍历全体数据的高计算开销，特别适合处理大规模数据集和深度学习场景。SGD的核心优势在于高效性和灵活性——单次迭代速度快、内存占用低，且梯度噪声带来的随机性有助于模型跳出局部最优解，但也存在更新方向不稳定、收敛路径震荡以及对学习率敏感等问题。实际应用中常通过引入动量（加速收敛、减少震荡）、自适应学习率（如Adam动态调整参数更新幅度）或学习率调度策略（如预热和余弦退火）来优化其性能。当前SGD及其变体（如分布式SGD、LAMB优化器）仍是训练神经网络、推荐系统和自然语言处理模型的主流方法，尤其在超大规模模型训练中，通过梯度裁剪、权重衰减等技巧进一步提升稳定性和泛化能力。

### 过拟合、欠拟合

- **过拟合**指模型过度适配训练数据中的噪声或偶然规律（如记忆样本而非学习特征），表现为训练集精度极高但测试集表现显著下降，常见于模型复杂度过高（如深度神经网络层数过多）或数据量不足时，可通过正则化（L1/L2）、数据增强、早停法或简化模型结构缓解；
- **欠拟合**指模型未能充分捕捉数据中的基本规律，表现为训练集与测试集性能均较差，通常因模型过于简单（如线性模型拟合非线性关系）、特征工程不足（如未提取有效交互项）或训练不充分导致，需通过增加模型复杂度（如引入多项式特征、使用树模型或深度网络）、优化特征表达（如分桶、编码）或延长训练时间改进；



## 逻辑回归与线性回归

### 什么是逻辑回归？

逻辑回归是一种基于概率的线性分类模型，主要用于解决二分类问题（如预测用户是否点击广告或患病风险），其核心通过Sigmoid函数将线性组合 $y = \theta^T x$ 映射到[0,1]区间，输出样本属于正类的概率（例如“用户点击概率为80%”），训练时通过极大似然估计和交叉熵损失函数优化参数，支持L1/L2正则化防止过拟合，优势在于计算高效、可解释性强（权重直接反映特征影响程度）

- **Sigmoid函数** 由于线性回归的结果范围为正负无穷，因此通过**对数几率**将线性回归的**预测结果**非线性映射到固定区间(0~1)之间，数学表达式为: $\text{Sigmod}(z) = \cfrac{1}{1+e^{-z}}$

逻辑回归的极大似然估计（MLE）通过最大化样本数据出现的概率来求解参数：首先假设标签服从伯努利分布，构建似然函数$L(w)=\prod \sigma(w^T x_i)^{y_i}(1-\sigma(w^T x_i))^{1-y_i}$，取对数转化为交叉熵损失$J(w)=-\frac{1}{N}\sum [y_i\log\sigma(w^T x_i)+(1-y_i)\log(1-\sigma(w^T x_i))]$，其梯度为$\nabla_w J=\frac{1}{N}\sum (\sigma(w^T x_i)-y_i)x_i$，通过梯度下降（如SGD、Adam）或牛顿法（利用Hessian矩阵加速收敛）迭代优化参数；为提升泛化能力，常加入L1/L2正则化约束权重，前者稀疏化特征，后者抑制过拟合。实际应用中需特征标准化、参数初始化为近零值，并监控验证损失防止早停，其高效线性计算与概率可解释性使其在金融风控、医疗诊断等场景中不可替代。

最大化对数似然等价于最小化**交叉熵损失函数**



### 逻辑回归为什么不用最小二乘？

逻辑回归不使用最小二乘法（OLS）（或者均方误差）的核心原因在于模型假设与优化目标的本质冲突：OLS假设误差服从正态分布且输出为连续值，而逻辑回归建模的是二分类概率（伯努利分布），若强行使用OLS，会导致损失函数因Sigmoid非线性映射而非凸（存在多个局部极小），且梯度计算中包含$\sigma'(z)$项（当预测接近0/1时梯度消失），难以有效更新参数；此外，OLS的均方误差对极端概率预测（如真实标签1但预测概率0.9）惩罚不足（残差平方仅0.01），而交叉熵损失通过$\log$函数对错误预测施加指数级惩罚（如$-\log(0.1)=2.3$），更适配概率建模需求，同时MLE（极大似然估计）保证了参数估计的一致性，避免概率值超出[0,1]的合理范围，而OLS可能产生荒谬预测（如概率120%）。因此，逻辑回归选择交叉熵损失是理论适配性、优化稳定性与业务解释性的综合最优解。





### 逻辑回归采用的是交叉熵，那你知道相对熵（KL）吗？





## 正则化

### L1正则化和L2正则化有哪些优缺点？简要介绍L1正则化和L2正则化

> 蔚来多模态一面；快手推荐算法一面

L1和L2正则化方法都可以对模型的损失函数引入一个惩罚项，在一定程度上起到防止过拟合的效果。

- **L1正则化**通过添加权重的绝对值之和（$\lambda||w||_1$）实现**稀疏解**，能自动进行特征选择（将不重要特征的权重压缩至0），适用于高维数据（如基因序列分析、文本分类）且需模型可解释的场景，但其缺点是对高度相关特征的选择不稳定，且优化时因绝对值函数的非光滑性需特殊算法（如坐标下降）；
- **L2正则化**通过添加权重的平方和（$\lambda||w||_2^2$）实现**参数平滑**，抑制极端权重值以提升泛化性（如防止神经网络过拟合），其损失函数始终可导且优化高效，适用于特征相关性高且需稳定输出的场景（如推荐系统、图像识别），但无法做特征筛选且对噪声敏感；

| **维度**       | L1正则化（特征选择）           | L2正则化（权重平滑）         |
| -------------- | ------------------------------ | ---------------------------- |
| **惩罚项形式** | $\lambda||w||_1$（绝对值之和） | $\lambda||w||_2^2$（平方和） |
| **解的性质**   | 稀疏解（部分权重精确为零）     | 稠密解（权重接近零但不归零） |
| **优化复杂度** | 需特殊算法处理非光滑性         | 标准梯度下降可直接优化       |
| **适用场景**   | 高维数据特征筛选               | 防止过拟合，通用模型稳定     |

### L1正则化比L2正则化更容易得到稀疏解的原因

> 蔚来多模态一面、vivo一面

从数学视角，L1正则化的绝对值惩罚（$\lambda||w||_1$）在参数空间中形成“菱形”约束域（高维超立方体），其顶点位于坐标轴上，当损失函数等高线与约束域相交时，最优解往往出现在顶点处（即部分权重被强制归零）；而L2正则化的平方惩罚（$\lambda||w||_2^2$）形成“圆形”约束域，最优解通常位于坐标轴之间的平滑区域，权重趋近于小值但难以精确为零。

从梯度行为看，L1对权重更新的惩罚是固定符号的（$\text{sign}(w)$），当权重接近零时仍保持恒定梯度，推动其归零；而L2的梯度与权重值成正比（$2λw$），权重越小梯度越弱，最终仅将权重压缩至接近零而非精确零。

此外，L1正则化对应拉普拉斯先验分布，天然偏好稀疏参数，而L2对应高斯先验，倾向均匀小权重，因此L1在特征选择、模型压缩等需明确稀疏性的场景中更具优势。

### 正则化解决过拟合

通过向损失函数添加惩罚项（如L1/L2正则化的权重范数），直接限制模型参数的自由度，迫使权重趋向更小、更平滑的值（如L2正则化将极端权重压缩至接近零），减少模型对训练数据中噪声或特异性的敏感度；

L2正则化可以防止过拟合的原因在于它对模型中的权重进行约束，使权重趋向于较小的值。具体来说，L2正则化在模型优化的过程中，会在损失函数中增加对模型权重平方的正则化惩罚项，使得模型在训练时更加关注权重的大小，并将较大的权重进行惩罚，从而降低了过拟合的风险。至于为什么权重大就倾向于会过拟合，参考第二点。

通过引入正则化项，L2正则化可以有效控制模型过于复杂，过拟合的情况。同时，L2正则化还可以使模型具有更好的泛化能力，从而适用于更多的数据集。总之，L2正则化通过对权重进行约束，降低模型复杂度，减少过拟合的风险，提高模型的泛化能力。

为什么权重大就更可能导致过拟合？

1. 过大的权重会导致模型对于训练数据的细节特征过度拟合，而无法泛化到新的数据上，导致过拟合。
2. 过大的权重也会导致模型的复杂度增加，这会增加模型对于数据的拟合能力，但同时也会降低模型的泛化能力，因为模型可能会学习到训练数据的噪声和不必要的特征。
3. 过大的权重也会导致模型的优化难度加大，因为梯度下降等优化算法可能会因为权重过大而无法收敛。



### Dropout

#### 为什么Dropout可以抑制过拟合？它的工作原理和实现？

Dropout是指在深度网络的训练中，以一定的概率随机地 “临时丢弃”一部分神经元节点。具体来讲，Dropout作用于每份小批量训练数据，由于其随机丢弃部分神经元的机制，相当于每次迭代都在训练不同结构的神经网络。**类比于Bagging方法**，Dropout可被认为是一种实用的大规模深度神经网络的模型集成算法。这是由于传统意义上的Bagging涉及多个模型的同时训练与测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout在小批量级别上的操作，提供了一种轻量级的Bagging集成近似，能够实现指数级数量神经网络的训练与评测。

Dropout的具体实现中，要求某个神经元节点激活值以一定的概率*p*被“丢弃”，即该神经元暂时停止工作，如图9.12所示。因此，对于包含*N*个神经元节点的网络，在Dropout的作用下可看作为 $2^N$ 个模型的集成。这 $2^N$ 个模型可认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数，而模型整体的参数数目不变，这就大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减少过拟合的风险，增强泛化能力。

#### Dropout和BN在神经网络中 先后顺序是什么，分析应该先dropout还是先bn

> paypal算法-2024

神经网络中，**Dropout与Batch Normalization（BN）的顺序**需基于**数据分布稳定性**与**正则化有效性**权衡：通常推荐**先进行BN再应用Dropout**，原因如下：

1. **BN的作用优先性**：BN通过对每层输入进行标准化（减均值、除标准差），强制数据分布稳定，缓解梯度消失/爆炸，若先使用Dropout（随机屏蔽神经元），会破坏数据分布的一致性，导致BN的均值和方差估计偏差；

#### Dropout在训练和推理时分别使用有什么目的，效果有什么不同

> paypal算法-2024、阿里-2022、pdd-2023

训练时，Dropout以概率p*p*随机屏蔽神经元并缩放剩余神经元的输出（保持期望值稳定），通过强制网络不依赖特定神经元间的协同适应来增强泛化能力，本质是一种动态集成与噪声注入；推理时，关闭Dropout并启用完整网络结构，同时将所有权重乘以1−p1−*p*以等价补偿训练时的缩放操作，确保输出稳定性和计算效率。

## 神经网络

