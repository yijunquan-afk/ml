## 介绍一下随机梯度下降

> 中兴算法岗一面

​     随机梯度下降（SGD）是一种通过随机采样单个样本或小批量数据来计算梯度并更新模型参数的优化算法，它避免了传统批量梯度下降中需要遍历全体数据的高计算开销，特别适合处理大规模数据集和深度学习场景。SGD的核心优势在于高效性和灵活性——单次迭代速度快、内存占用低，且梯度噪声带来的随机性有助于模型跳出局部最优解，但也存在更新方向不稳定、收敛路径震荡以及对学习率敏感等问题。实际应用中常通过引入动量（加速收敛、减少震荡）、自适应学习率（如Adam动态调整参数更新幅度）或学习率调度策略（如预热和余弦退火）来优化其性能。当前SGD及其变体（如分布式SGD、LAMB优化器）仍是训练神经网络、推荐系统和自然语言处理模型的主流方法，尤其在超大规模模型训练中，通过梯度裁剪、权重衰减等技巧进一步提升稳定性和泛化能力。



## 正则化

### L1正则化和L2正则化有哪些优缺点？简要介绍L1正则化和L2正则化

> 蔚来多模态一面

L1和L2正则化方法都可以对模型的损失函数引入一个惩罚项，在一定程度上起到防止过拟合的效果。

- **L1正则化**通过添加权重的绝对值之和（$\lambda||w||_1$）实现**稀疏解**，能自动进行特征选择（将不重要特征的权重压缩至0），适用于高维数据（如基因序列分析、文本分类）且需模型可解释的场景，但其缺点是对高度相关特征的选择不稳定，且优化时因绝对值函数的非光滑性需特殊算法（如坐标下降）；
- **L2正则化**通过添加权重的平方和（$\lambda||w||_2^2$）实现**参数平滑**，抑制极端权重值以提升泛化性（如防止神经网络过拟合），其损失函数始终可导且优化高效，适用于特征相关性高且需稳定输出的场景（如推荐系统、图像识别），但无法做特征筛选且对噪声敏感；

| **维度**       | L1正则化（特征选择）           | L2正则化（权重平滑）         |
| -------------- | ------------------------------ | ---------------------------- |
| **惩罚项形式** | $\lambda||w||_1$（绝对值之和） | $\lambda||w||_2^2$（平方和） |
| **解的性质**   | 稀疏解（部分权重精确为零）     | 稠密解（权重接近零但不归零） |
| **优化复杂度** | 需特殊算法处理非光滑性         | 标准梯度下降可直接优化       |
| **适用场景**   | 高维数据特征筛选               | 防止过拟合，通用模型稳定     |

### L1正则化比L2正则化更容易得到稀疏解的原因

> 蔚来多模态一面、vivo一面

从数学视角，L1正则化的绝对值惩罚（$\lambda||w||_1$）在参数空间中形成“菱形”约束域（高维超立方体），其顶点位于坐标轴上，当损失函数等高线与约束域相交时，最优解往往出现在顶点处（即部分权重被强制归零）；而L2正则化的平方惩罚（$\lambda||w||_2^2$）形成“圆形”约束域，最优解通常位于坐标轴之间的平滑区域，权重趋近于小值但难以精确为零。

从梯度行为看，L1对权重更新的惩罚是固定符号的（$\text{sign}(w)$），当权重接近零时仍保持恒定梯度，推动其归零；而L2的梯度与权重值成正比（$2λw$），权重越小梯度越弱，最终仅将权重压缩至接近零而非精确零。

此外，L1正则化对应拉普拉斯先验分布，天然偏好稀疏参数，而L2对应高斯先验，倾向均匀小权重，因此L1在特征选择、模型压缩等需明确稀疏性的场景中更具优势。





## 逻辑回归与线性回归

### 什么是逻辑回归？

逻辑回归是一种基于概率的线性分类模型，主要用于解决二分类问题（如预测用户是否点击广告或患病风险），其核心通过Sigmoid函数将线性组合 $y = \theta^T x$ 映射到[0,1]区间，输出样本属于正类的概率（例如“用户点击概率为80%”），训练时通过极大似然估计和交叉熵损失函数优化参数，支持L1/L2正则化防止过拟合，优势在于计算高效、可解释性强（权重直接反映特征影响程度）

- **Sigmoid函数** 由于线性回归的结果范围为正负无穷，因此通过**对数几率**将线性回归的**预测结果**非线性映射到固定区间(0~1)之间，数学表达式为: $\text{Sigmod}(z) = \cfrac{1}{1+e^{-z}}$

逻辑回归的极大似然估计（MLE）通过最大化样本数据出现的概率来求解参数：首先假设标签服从伯努利分布，构建似然函数$L(w)=\prod \sigma(w^T x_i)^{y_i}(1-\sigma(w^T x_i))^{1-y_i}$，取对数转化为交叉熵损失$J(w)=-\frac{1}{N}\sum [y_i\log\sigma(w^T x_i)+(1-y_i)\log(1-\sigma(w^T x_i))]$，其梯度为$\nabla_w J=\frac{1}{N}\sum (\sigma(w^T x_i)-y_i)x_i$，通过梯度下降（如SGD、Adam）或牛顿法（利用Hessian矩阵加速收敛）迭代优化参数；为提升泛化能力，常加入L1/L2正则化约束权重，前者稀疏化特征，后者抑制过拟合。实际应用中需特征标准化、参数初始化为近零值，并监控验证损失防止早停，其高效线性计算与概率可解释性使其在金融风控、医疗诊断等场景中不可替代。

最大化对数似然等价于最小化**交叉熵损失函数**



### 逻辑回归为什么不用最小二乘？

逻辑回归不使用最小二乘法（OLS）的核心原因在于模型假设与优化目标的本质冲突：OLS假设误差服从正态分布且输出为连续值，而逻辑回归建模的是二分类概率（伯努利分布），若强行使用OLS，会导致损失函数因Sigmoid非线性映射而非凸（存在多个局部极小），且梯度计算中包含$\sigma'(z)$项（当预测接近0/1时梯度消失），难以有效更新参数；此外，OLS的均方误差对极端概率预测（如真实标签1但预测概率0.9）惩罚不足（残差平方仅0.01），而交叉熵损失通过$\log$函数对错误预测施加指数级惩罚（如$-\log(0.1)=2.3$），更适配概率建模需求，同时MLE（极大似然估计）保证了参数估计的一致性，避免概率值超出[0,1]的合理范围，而OLS可能产生荒谬预测（如概率120%）。因此，逻辑回归选择交叉熵损失是理论适配性、优化稳定性与业务解释性的综合最优解。





### 逻辑回归缓解过拟合的方法





### 逻辑回归采用的是交叉熵，那你知道相对熵（KL）吗？

