【从编解码和词嵌入开始，一步一步理解Transformer，注意力机制(Attention)的本质是卷积神经网络(CNN)】 https://www.bilibili.com/video/BV1XH4y1T76e/?share_source=copy_web&vd_source=d68e768cc379452328c0e9693a144c4c



隐藏层越深，抽象程度越高

