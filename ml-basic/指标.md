## 机器学习中常用的指标

### 分类模型

#### Acc｜Prec|Recall|F1-score

对于二分类问题，有分类结果混淆矩阵

![image-20220303111021487](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img4/1695721684.png)

> True Positive，即正确预测出的正样本个数
>
> False Positive，即错误预测出的正样本个数（本来是负样本，被我们预测成了正样本）
>
> True Negative，即正确预测出的负样本个数
>
> False Negative，即错误预测出的负样本个数（本来是正样本，被我们预测成了负样本）

**准确率(Accuracy)**＝(TP + TN)/总样本

> 定义是: 对于给定的测试数据集，分类器正确分类的样本数与总样本数之比

**精确率(Precision)**＝ TP /(TP + FP)，又称为查准率

> 预测为正的样本中有多少是真正的正样本，是**针对我们预测结果**而言的

**召回率(Recall)**＝ TP /(TP + FN)，又称为查全率

> 它表示：样本中的正例有多少被预测正确了，是**针对我们原来的样本**而言的

**F1-score**：F1-score 是精确率和召回率的调和平均数，计算公式为：
$$
\text{F1-score}=2×\cfrac{Precision+Recall}{Precision×Recall}
$$


F1-score 的取值范围是 [0, 1]，值越高表示模型的分类效果越好。F1-score 考虑了精确率和召回率的平衡，因此**在正负样本分布不均的情况下，F1-score 是一个比准确率（Accuracy）更合适的评价指标**。

#### ROC与AUC

ROC曲线和AUC是评估==二分类模型==性能的重要工具。ROC曲线直观地展示了模型在不同阈值下的性能，而AUC值则提供了一个简洁且鲁棒的性能指标。在实际应用中，结合ROC和AUC可以更好地理解和优化模型的分类能力。

##### ROC曲线
- **定义**：ROC曲线以**假正率（FPR）**为横轴，**真正率（TPR）**为纵轴，展示模型在不同阈值下的分类性能。
- **意义**：曲线越接近左上角，模型性能越好。
- **公式**：
  - TPR（召回率）：$\frac{TP}{TP + FN}$
  - FPR：$\frac{FP}{FP + TN}$

##### AUC（Area Under Curve）
- **定义**：AUC是ROC曲线下方的面积，用于量化模型的分类能力。
- **取值范围**：0到1，值越大表示性能越好。
- **意义**：
  - AUC = 1：完美分类器。
  - AUC > 0.5：优于随机猜测。
  - AUC = 0.5：随机猜测。
  - AUC < 0.5：性能较差。

##### 优点
- **直观性**：通过曲线和面积直观评估模型性能。
- **鲁棒性**：对类别不平衡数据具有一定的鲁棒性。

##### 局限性
- 在**极度不平衡数据**中，AUC可能无法完全反映模型的实际性能，需结合其他指标（如Precision、Recall）进行综合评估。

##### 应用
- **模型选择**：通过比较AUC值选择性能更好的模型。
- **超参数调优**：优化模型参数以提高AUC值。
- **性能评估**：在模型部署前评估分类能力。