# ML2 ä½¿ç”¨æ¢¯åº¦ä¸‹é™çš„çº¿æ€§å›å½’
[ç‰›å®¢ç½‘](https://www.nowcoder.com/practice/e9f12bb403f44847b44e287d5a71e56c?tpId=379&tqId=11118316&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DAI%25E7%25AF%2587%26topicId%3D379)

## æè¿°
ç¼–å†™ä¸€ä¸ªä½¿ç”¨æ¢¯åº¦ä¸‹é™æ‰§è¡Œçº¿æ€§å›å½’çš„ Python å‡½æ•°ã€‚è¯¥å‡½æ•°åº”å°† NumPy æ•°ç»„ Xï¼ˆå…·æœ‰ä¸€åˆ—æˆªè·çš„ç‰¹å¾ï¼‰å’Œ yï¼ˆç›®æ ‡ï¼‰ä½œä¸ºè¾“å…¥ï¼Œä»¥åŠå­¦ä¹ ç‡ alpha å’Œè¿­ä»£æ¬¡æ•°ï¼Œå¹¶è¿”å›ä¸€ä¸ª NumPy æ•°ç»„ï¼Œè¡¨ç¤ºçº¿æ€§å›å½’æ¨¡å‹çš„ç³»æ•°ã€‚
## è¾“å…¥æè¿°ï¼š
ç¬¬1è¡Œè¾“å…¥Xï¼Œç¬¬2è¡Œè¾“å…¥yï¼Œç¬¬3è¡Œè¾“å…¥alphaï¼Œç¬¬4è¡Œè¾“å…¥è¿­ä»£æ¬¡æ•°ã€‚

## è¾“å‡ºæè¿°ï¼š
è¾“å‡ºçº¿æ€§å›å½’æ¨¡å‹çš„ç³»æ•°ï¼Œå››èˆäº”å…¥åˆ°å°æ•°ç‚¹åå››ä½ã€‚è¿”å›ç±»å‹æ˜¯Listç±»å‹ã€‚

```
è¾“å…¥:
[[1, 1], [1, 2], [1, 3], [1, 4]]
[2, 3, 4, 5]
0.01
1000

è¾“å‡º: 
[0.8678 1.045 ]
```


```python
import numpy as np
def linear_regression_gradient_descent(X, y, alpha, iterations):
    # è¡¥å…¨ä»£ç 
    m,n = X.shape
    theta = np.zeros((n,1)) # ä¸ºäº†å’Œç­”æ¡ˆä¸€è‡´
    for _ in range(iterations):
        y_predict = X@theta
        errors = y_predict - y
        discent = X.T@(errors)/m
        theta = theta - alpha * discent
    return np.round(theta.flatten(), 4)

# ä¸»ç¨‹åº
if __name__ == "__main__":
    # è¾“å…¥çŸ©é˜µå’Œå‘é‡
    matrix_inputx = input()
    array_y = input()
    alpha = input()
    iterations = input()

    # å¤„ç†è¾“å…¥
    import ast
    matrix = np.array(ast.literal_eval(matrix_inputx))
    y = np.array(ast.literal_eval(array_y)).reshape(-1,1)
    alpha = float(alpha)
    iterations = int(iterations)

    # è°ƒç”¨å‡½æ•°è®¡ç®—é€†çŸ©é˜µ
    output = linear_regression_gradient_descent(matrix,y,alpha,iterations)
    
    # è¾“å‡ºç»“æœ
    print(output)


```

    [0.8678 1.045 ]


## æ¢¯åº¦ä¸‹é™æ±‚è§£

æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§è®¡ç®—å±€éƒ¨æœ€å°å€¼çš„ä¸€ç§æ–¹æ³•ã€‚æ¢¯åº¦ä¸‹é™æ€æƒ³å°±æ˜¯ç»™å®šä¸€ä¸ªåˆå§‹å€¼ğœƒï¼Œæ¯æ¬¡æ²¿ç€å‡½æ•°æ¢¯åº¦ä¸‹é™çš„æ–¹å‘ç§»åŠ¨ğœƒï¼š

$$
\theta^{(t+1)} := \theta^{(t)} - \alpha \nabla_{\theta} J(\theta^{(t)})
$$


åœ¨æ¢¯åº¦ä¸ºé›¶æˆ–è¶‹è¿‘äºé›¶çš„æ—¶å€™æ”¶æ•›
$$
J(\theta)=\frac{1}{2n}\sum^n_{i=1}(x_i^T\theta-y_i)^2
$$
å¯¹æŸå¤±å‡½æ•°æ±‚åå¯¼å¯å¾—åˆ° (nä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬pç»´)
$$
x_i=(x_{i,0},...,x_{i,p})^T\\
x_{ij}è¡¨ç¤ºç¬¬iä¸ªæ ·æœ¬çš„ç¬¬jä¸ªåˆ†é‡\\
\frac{\partial}{\theta_j}\frac{1}{2n}(x_i^T\theta-y_i)^2=\frac{\partial}{\theta_j}\frac{1}{2n}(\sum^p_{j=0}x_{i,j}\theta_j-y_i)^2=\frac{1}{n}(\sum^p_{j=0}x_{i,j}\theta_j-y_i)x_{i,j}=\frac{1}{n}(f(x_i)-y_i))x_{i,j}
\\
\nabla_\theta J=\begin{bmatrix}
\frac{J}{\theta_0}\\
\frac{J}{\theta_1}\\...\\
\frac{J}{\theta_p}
\end{bmatrix}
$$
å¯¹äºåªæœ‰ä¸€ä¸ªè®­ç»ƒæ ·æœ¬çš„è®­ç»ƒç»„è€Œè¨€ï¼Œæ¯èµ°ä¸€æ­¥ï¼Œğœƒğ‘—(ğ‘—= 0,1,â€¦,ğ‘)çš„æ›´æ–°å…¬å¼å°±å¯ä»¥å†™æˆï¼š
$$
\theta_j^{(t+1)} := \theta_j^{(t)} - \alpha \frac{\partial}{\partial \theta_j} J(\theta_j^{(t)}) = \theta_j^{(t)} - \alpha \frac{1}{n} (f(x_i) - y_i) x_{i,j}
$$
å› æ­¤ï¼Œå½“æœ‰ n ä¸ªè®­ç»ƒå®ä¾‹çš„æ—¶å€™ï¼ˆæ‰¹å¤„ç†æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼‰ï¼Œè¯¥å…¬å¼å°±å¯ä»¥å†™ä¸ºï¼š
$$
\theta_j^{(t+1)}:=\theta_j^{(t)}-\alpha\frac{1}{n}\sum^n_{i=1}(f(x_i)-y_i)x_{i,j}
$$
è¿™æ ·ï¼Œæ¯æ¬¡æ ¹æ®æ‰€æœ‰æ•°æ®æ±‚å‡ºåå¯¼ï¼Œç„¶åæ ¹æ®ç‰¹å®šçš„æ­¥é•¿ğ›¼ï¼Œå°±å¯ä»¥ä¸æ–­æ›´æ–°ğœƒğ‘—ï¼Œç›´åˆ°å…¶æ”¶æ•›ã€‚å½“<mark>æ¢¯åº¦ä¸º0æˆ–ç›®æ ‡å‡½æ•°å€¼ä¸èƒ½ç»§ç»­ä¸‹é™çš„æ—¶å€™</mark>ï¼Œå°±å¯ä»¥è¯´å·²ç»æ”¶æ•›ï¼Œå³ç›®æ ‡å‡½æ•°è¾¾åˆ°å±€éƒ¨æœ€å°å€¼ã€‚

å…·ä½“è¿‡ç¨‹å¯ä»¥å½’çº³å¦‚ä¸‹

> :one: åˆå§‹åŒ–ğœƒï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
>
> :two: åˆ©ç”¨å¦‚ä¸‹å…¬å¼æ›´æ–°ğœƒ
> $$
> \theta_j^{(t+1)}:=\theta_j^{(t)}-\alpha \frac{1}{n}\sum^n_{i=1}(f(x_i)-y_i)x_{i,j}\\
> \theta^{(t+1)}:=\theta^{(t)}-\alpha \frac{1}{n}\sum^n_{i=1}(f(x_i)-y_i)x_{i}
> $$
> å…¶ä¸­Î±ä¸ºæ­¥é•¿
>
> :three: å¦‚æœæ–°çš„ğœƒèƒ½ä½¿ğ½(ğœƒ)ç»§ç»­å‡å°‘ï¼Œç»§ç»­åˆ©ç”¨ä¸Šè¿°æ­¥éª¤æ›´æ–°ğœƒï¼Œå¦åˆ™æ”¶æ•›ï¼Œåœæ­¢è¿­ä»£ã€‚
>


